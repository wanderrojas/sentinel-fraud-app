"""
Debate Agents
Pro-Fraud Agent vs Pro-Customer Agent
"""
from app.services.llm_service import get_llm
from langchain_core.prompts import ChatPromptTemplate
from typing import Dict, List


class DebateAgents:
    """
    Sistema de debate: Pro-Fraud vs Pro-Customer
    """
    
    def __init__(self):
        self.llm = get_llm(temperature=0.5)
        self.name = "Debate Agents"
    
    def analyze(
        self,
        transaction_id: str,
        all_signals: List[str],
        aggregated_risk_score: float,
        citations_internal: List,
        citations_external: List
    ) -> Dict:
        """
        Ejecutar debate entre agentes Pro-Fraud y Pro-Customer
        y decidir acci√≥n final mediante Decision Arbiter
        
        Returns:
            Dict con argumentos de debate, decisi√≥n final y explicaciones
        """
        
        print(f"\nü§ñ {self.name} iniciando debate...")
        
        # ============================================
        # PASO 1: CONSTRUIR CONTEXTO
        # ============================================
        
        context = f"""
TRANSACCI√ìN: {transaction_id}
RISK SCORE: {aggregated_risk_score:.2f}

SE√ëALES DETECTADAS ({len(all_signals)}):
{chr(10).join(f'- {signal}' for signal in all_signals[:10])}

POL√çTICAS CITADAS: {len(citations_internal)}
ALERTAS EXTERNAS: {len(citations_external)}
"""
        
        # ============================================
        # PASO 2: GENERAR ARGUMENTOS DE DEBATE
        # ============================================
        
        debate_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un sistema de an√°lisis de fraude que presenta AMBOS lados del argumento.

Genera DOS argumentos balanceados y objetivos:

1. PRO-FRAUD (Por qu√© ES fraude):
Argumenta fuertemente que esta transacci√≥n ES fraudulenta, bas√°ndote en las se√±ales detectadas.
S√© espec√≠fico con los datos (montos, porcentajes, etc.)

2. PRO-CUSTOMER (Por qu√© NO es fraude):
Argumenta fuertemente que esta transacci√≥n es LEG√çTIMA, buscando explicaciones razonables.
Considera escenarios normales que podr√≠an explicar las anomal√≠as.

Formato:
PRO-FRAUD: [2-3 l√≠neas]

PRO-CUSTOMER: [2-3 l√≠neas]"""),
            ("user", "{context}")
        ])
        
        print("   üì° Generando argumentos de debate...")
        debate_response = self.llm.invoke(debate_prompt.format_messages(context=context))
        
        # Parsear argumentos
        content = debate_response.content
        pro_fraud_arg = ""
        pro_customer_arg = ""
        
        if "PRO-FRAUD" in content and "PRO-CUSTOMER" in content:
            parts = content.split("PRO-CUSTOMER")
            pro_fraud_part = parts[0].replace("PRO-FRAUD", "").replace("1.", "").strip()
            pro_customer_part = parts[1].replace("2.", "").strip()
            
            # Limpiar
            pro_fraud_arg = pro_fraud_part.strip(": ").strip()
            pro_customer_arg = pro_customer_part.strip(": ").strip()
        
        print(f"   ‚úÖ Debate completado")
        
        # ============================================
        # PASO 3: DECISION ARBITER (IA DECIDE)
        # ============================================
        
        print("   ‚öñÔ∏è  Decision Arbiter evaluando...")
        
        final_decision = self._decision_arbiter(
            aggregated_risk_score=aggregated_risk_score,
            pro_fraud_argument=pro_fraud_arg,
            pro_customer_argument=pro_customer_arg,
            signals=all_signals,
            policies=citations_internal,
            threats=citations_external
        )
        
        print(f"   ‚úÖ Decisi√≥n final: {final_decision}")
        
        # ============================================
        # PASO 4: GENERAR EXPLICACIONES
        # ============================================
        
        print("   üìù Generando explicaciones...")
        
        # Generar explicaci√≥n para el cliente
        customer_explanation = self._generate_customer_explanation(
            final_decision, aggregated_risk_score, all_signals
        )
        
        # Generar explicaci√≥n para auditor√≠a
        audit_explanation = self._generate_audit_explanation(
            transaction_id, final_decision, aggregated_risk_score, all_signals,
            citations_internal, citations_external
        )
        
        print("   ‚úÖ Explicaciones generadas")
        
        # ============================================
        # RETORNO COMPLETO
        # ============================================
        
        return {
            "agent": self.name,
            "debate_summary": debate_response.content,
            "pro_fraud_argument": pro_fraud_arg,
            "pro_customer_argument": pro_customer_arg,
            "decision_recommendation": final_decision,  # ‚Üê DECISI√ìN DE IA
            "explanation_customer": customer_explanation,
            "explanation_audit": audit_explanation
        }
    
    def _decision_arbiter(
        self,
        aggregated_risk_score: float,
        pro_fraud_argument: str,
        pro_customer_argument: str,
        signals: List[str],
        policies: List,
        threats: List
    ) -> str:
        """
        Decision Arbiter: La IA toma la decisi√≥n final bas√°ndose en el debate
        """
        
        # Preparar contexto para el Arbiter
        context = f"""
RISK SCORE CALCULADO: {aggregated_risk_score:.2f} (0.0 = sin riesgo, 1.0 = muy alto riesgo)

ARGUMENTO PRO-FRAUD (por qu√© podr√≠a SER fraude):
{pro_fraud_argument}

ARGUMENTO PRO-CUSTOMER (por qu√© podr√≠a ser LEG√çTIMO):
{pro_customer_argument}

INFORMACI√ìN ADICIONAL:
- Total de se√±ales detectadas: {len(signals)}
- Pol√≠ticas internas aplicadas: {len(policies)}
- Alertas de amenazas externas: {len(threats)}

POL√çTICAS APLICADAS:
{chr(10).join(f"- {p.policy_id if hasattr(p, 'policy_id') else p.get('policy_id')}" for p in policies) if policies else "- Ninguna"}
"""
        
        # Prompt para el Decision Arbiter
        arbiter_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres el Decision Arbiter del Sistema de Detecci√≥n de Fraude del BCP.

Tu responsabilidad es tomar la DECISI√ìN FINAL sobre cada transacci√≥n.

CONTEXTO IMPORTANTE:
- El sistema ya calcul√≥ un risk score (0.0 = sin riesgo, 1.0 = alto riesgo)
- Ya analiz√≥ 6 agentes especializados
- Ya aplic√≥ todas las pol√≠ticas bancarias
- Ya consult√≥ amenazas externas

Tu trabajo es CONFIRMAR o AJUSTAR esa evaluaci√≥n bas√°ndote en el contexto completo.

OPCIONES DE DECISI√ìN:

**APPROVE**
- Usar cuando la transacci√≥n es claramente leg√≠tima
- EJEMPLOS DE CU√ÅNDO APROBAR:
  * Risk score < 0.20
  * Behavioral score >= 0.95
  * Sin pol√≠ticas aplicadas
  * Sin amenazas externas
  * Monto dentro de ¬±50% del promedio
  * Dispositivo conocido, pa√≠s habitual, horario normal

**CHALLENGE**
- Usar cuando hay dudas razonables
- EJEMPLOS DE CU√ÅNDO DESAFIAR:
  * Risk score 0.35-0.60
  * 2+ anomal√≠as moderadas juntas
  * Monto muy alto (3x+) aunque todo lo dem√°s sea normal
  * Horario muy inusual (madrugada) aunque solo eso sea raro

**BLOCK**
- Usar cuando hay alto riesgo de fraude
- EJEMPLOS DE CU√ÅNDO BLOQUEAR:
  * Risk score > 0.70
  * Monto alto + pa√≠s diferente + dispositivo nuevo
  * Comercio en lista negra + m√∫ltiples anomal√≠as

**ESCALATE_TO_HUMAN**
- Usar para casos complejos
- EJEMPLOS DE CU√ÅNDO ESCALAR:
  * Risk score 0.60-0.80 con se√±ales contradictorias
  * M√∫ltiples pol√≠ticas aplicadas
  * Amenazas cr√≠ticas + otras se√±ales

REGLAS ABSOLUTAS (NUNCA VIOLAR):

1. **Si risk score = 0.0 y behavioral score = 1.0 ‚Üí SIEMPRE APPROVE**
   - Esto significa: TODO est√° perfecto
   - No importa si hay se√±ales de "monitoreo" o "seguimiento"
   - Esas son sugerencias generales, no alarmas

2. **Si risk score < 0.20 y behavioral score >= 0.90 ‚Üí APPROVE**
   - A menos que haya pol√≠ticas cr√≠ticas aplicadas
   - O amenazas externas confirmadas

3. **MONTO BAJO (-30% o m√°s) NUNCA es cr√≠tico por s√≠ solo**
   - Cliente puede comprar algo peque√±o
   - No usar CHALLENGE solo por monto bajo

4. **Ignora frases gen√©ricas como:**
   - "Monitorear la transacci√≥n..."
   - "Realizar seguimiento..."
   - "Confirmar legitimidad..."
   - Estas son recomendaciones pasivas, NO alarmas

5. **Solo considera SE√ëALES CR√çTICAS:**
   - Monto > 3x promedio
   - Pa√≠s diferente al habitual
   - Dispositivo completamente nuevo
   - Horario de madrugada (2-5 AM)
   - Comercio en lista negra
   - Pol√≠ticas bancarias violadas

6. **Balance costo-beneficio:**
   - APPROVE cuando hay > 90% confianza de legitimidad
   - CHALLENGE solo cuando hay > 30% probabilidad de fraude
   - BLOCK solo cuando hay > 70% probabilidad de fraude

7. **Conf√≠a en los agentes anteriores:**
   - Si calcularon risk = 0.0, es porque analizaron TODO
   - No "inventes" riesgos que los agentes no detectaron

PROCESO DE DECISI√ìN:

Paso 1: ¬øRisk score < 0.20 Y behavioral score >= 0.90?
  ‚Üí S√ç: APPROVE (salvo pol√≠ticas/amenazas cr√≠ticas)
  ‚Üí NO: Continuar

Paso 2: ¬øHay pol√≠ticas bancarias aplicadas?
  ‚Üí S√ç: Seguir esas pol√≠ticas
  ‚Üí NO: Continuar

Paso 3: ¬øCu√°ntas se√±ales CR√çTICAS hay?
  ‚Üí 0-1: APPROVE
  ‚Üí 2: CHALLENGE
  ‚Üí 3+: ESCALATE o BLOCK

RESPONDE SOLO CON UNA PALABRA:
APPROVE
CHALLENGE
BLOCK
ESCALATE_TO_HUMAN

NO agregues explicaciones."""),
            ("user", "{context}")
        ])
        
        # Invocar IA para decidir
        response = self.llm.invoke(arbiter_prompt.format_messages(context=context))
        decision_text = response.content.strip().upper()
        
        # Validar respuesta
        valid_decisions = ["APPROVE", "CHALLENGE", "BLOCK", "ESCALATE_TO_HUMAN"]
        
        if decision_text in valid_decisions:
            return decision_text
        
        # Fallback
        print(f"   ‚ö†Ô∏è  Respuesta inesperada del Arbiter: '{decision_text}'. Usando fallback.")
        
        if aggregated_risk_score >= 0.75:
            return "BLOCK"
        elif aggregated_risk_score >= 0.55:
            return "ESCALATE_TO_HUMAN"
        elif aggregated_risk_score >= 0.35:
            return "CHALLENGE"
        else:
            return "APPROVE"
    
    
    def _generate_customer_explanation(
        self,
        decision: str,
        risk_score: float,
        signals: List[str]
    ) -> str:
        """Generar explicaci√≥n para el cliente"""
        
        context = f"""
DECISI√ìN: {decision}
RISK SCORE: {risk_score * 100:.0f}%

SE√ëALES PRINCIPALES:
{chr(10).join(f'- {signal}' for signal in signals[:3])}
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente del BCP que explica decisiones de seguridad al cliente.

Habla directamente al cliente sobre SU transacci√≥n. M√°ximo 2-3 l√≠neas (40-60 palabras).

APPROVE: "Su transacci√≥n ha sido aprobada exitosamente. Gracias por confiar en BCP."

CHALLENGE: "Por su seguridad, confirme esta operaci√≥n. Le enviaremos un c√≥digo por SMS debido a [motivo principal breve]."

BLOCK: "Bloqueamos esta transacci√≥n por su seguridad debido a [motivo principal]. Si fue usted, ll√°menos al 0800-100-2000."

ESCALATE_TO_HUMAN: "Su transacci√≥n est√° en revisi√≥n por seguridad debido a [motivo principal]. La validaremos en 5-10 minutos."

NO uses t√©rminos t√©cnicos como "IA", "risk score", "algoritmos". S√© claro, emp√°tico y conciso."""),
            ("user", "{context}")
        ])
        
        response = self.llm.invoke(prompt.format_messages(context=context))
        return response.content.strip()
    
    def _generate_audit_explanation(
        self,
        transaction_id: str,
        decision: str,
        risk_score: float,
        signals: List[str],
        policies: List,
        threats: List
    ) -> str:
        """Generar explicaci√≥n t√©cnica para auditor√≠a"""
        
        policy_ids = [
            p.policy_id if hasattr(p, 'policy_id') else p.get('policy_id')
            for p in policies
        ] if policies else []
        
        # Identificar factores clave autom√°ticamente
        key_factors = []
        signals_text = " ".join(signals).lower()
        
        if "monto" in signals_text and ("alto" in signals_text or "inusual" in signals_text or "elevado" in signals_text):
            key_factors.append("Monto elevado")
        if "horario" in signals_text and "at√≠pico" in signals_text:
            key_factors.append("Horario at√≠pico")
        if "dispositivo" in signals_text and ("nuevo" in signals_text or "desconocido" in signals_text):
            key_factors.append("Dispositivo no reconocido")
        if "pa√≠s" in signals_text and ("diferente" in signals_text or "internacional" in signals_text):
            key_factors.append("Ubicaci√≥n inusual")
        
        factors_text = ", ".join(key_factors) if key_factors else "M√∫ltiples factores"
        policies_text = ", ".join(policy_ids) if policy_ids else "Ninguna"
        threats_text = f"{len(threats)}" if threats else "0"
        
        return (
            f"Decisi√≥n: {decision} (Risk Score: {risk_score * 100:.0f}%) | "
            f"Factores: {factors_text} | "
            f"Pol√≠ticas: {policies_text} | "
            f"Alertas externas: {threats_text} | "
            f"Total se√±ales: {len(signals)}"
        )